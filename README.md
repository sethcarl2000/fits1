# fits1

## Exersice 1 
This is done with ```fit1a.C```. 
If you compare the std-dev of the experiment's reported mean to the fit's estimate of the mean's error (top right vs bottom right plots in ```result1.pdf```), you see that they generally agree with each other, but the estimate for the error is a little lower than the computed std-dev. I would guess this is a consequence of the fact that not all of the bins are patricipating in the fit, which reduces its quality (leading to outlying estimates of the mean, which disproportionately effect the std-dev.)

## Exersice 2 
As you can see, the log-liklihood method performs much better with the same level of statistics. As before the **difference** between the actual mean and the reconstructed mean is plotted. ```result2.pdf``` is generated with ```fit1b.C```. 

## Exersice 3 
Using the CDF in the right-plot in ```result3.pdf```, you can see that the P-value is ~ 0.40 (this of course is the probalility that a fit will be _worse_, given that it is indeed generated by this distribution). 

## Exersice 4 
```result4.pdf``` is generated with ```fit1d.C```. Each plot is the LL/chi2 as a function of its distance from the true mean. The vertical red line is the mean as reported by the fit, and the dotted red lines are the errors reported by the fit. You can clearly see where TMinuit would have made the cutoff to determine the error of a fit. 
